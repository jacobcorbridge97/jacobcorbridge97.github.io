[
  {
    "path": "posts/welcome/",
    "title": "Post #1: Welcome to Jacob Corbridge's Senior Blog",
    "description": "Kick back and stay a while.",
    "author": [
      {
        "name": "Jacob Corbridge",
        "url": "https://www.linkedin.com/in/jacobcorbridge/"
      }
    ],
    "date": "2021-07-21",
    "categories": [],
    "contents": "\r\nWelcome again to my Senior Project Blog. I am excited to go on this Data Science Journey, and to bring you along with me! There are limitless possibilities, but sadly there can only be one project. As I have learned from previous projects, sometimes the process itself can be the result. In data science, we often run into barriers, obstacles, or dead ends. This doesn’t kill the project, but rather changes it’s direction or focus. We’ll be seeing plenty of that over the next 14 weeks.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-15T16:02:56-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-28-first-post/",
    "title": "Post #2: Oh Deere",
    "description": "Creating a John Deere Aplication",
    "author": [
      {
        "name": "Jacob Corbridge",
        "url": "https://www.linkedin.com/in/jacobcorbridge/"
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\r\nMy name is Jacob Corbridge. I am a Data Analyst for a John Deere distributor called Stotz Equipment. I specialize in field maps and daily reporting. Field maps are easy to Generate using John Deere’s Operation Center. They look nice, too.\r\n\r\n\r\n\r\nWhile this is both beautiful and useful, only John Deere customers and employees have access to such data, and it does not come in raw form. The John Deere Operations Center does almost all of the heavy lifting in these cases, but there is a downside. Without raw data readily available, it is impossible to run statistical analysis or machine learning models on the data.\r\nI have thought a lot about this quandary and want to provide a solution so that Stotz Equipment and partners can have access to any raw data, at any time, for any reason. There are many paths that can be taken to provide such a result. Stotz already has a great pipeline setup for Tableau, but Raw data still cannot be accessed. I tried using the DBI package in R to connect to the database, but there simply wasn’t enough documentation to understand how to move forward.\r\nThe next option to turn to is the John Deere API. It has some detailed documentation (albeit confusing at times) and provides access to a wide range of data so long as you make the correct API call. Thankfully, Deere provided an example Python script to walk new users through the API process. The colab can be found here\r\nhttps://colab.research.google.com/github/JohnDeere/DevelopWithDeereNotebooks/blob/master/Onboarding/myjohndeere-api-onboarding.ipynb\r\nThe Colab begins from the top with the creation of a John Deere account and an Organization with sample data within the John Deere Operations Center. As an employee, Neither of those steps are necessary. The first step for me was to create an application that can be used to connect to the data necessary within the scope of the project. In order to do so, I accessed Develop with Deere and followed the basic steps to create an application called BYUI - API Connect. We can even check out the Operations Center Connect page to make sure it’s official.\r\n(https://developer-portal.deere.com/)\r\n\r\n\r\n\r\nWe can even check out the Operations Center Connect page to make sure it’s official.\r\n\r\n\r\n\r\nSome important security information is generated when an application is made. You are provided with an Application ID, a Client Secret (password), and you are also given the ability to state a redirect URI (a url that will provide an authentication password when called). For our purposes the link to Develop with Deere is all that is needed.\r\n\r\n\r\n\r\nAn item to note. Our new Application will only be granted Sandbox access, which is fine for our purposes. Production access is reserved exclusively for API calls that will be made on a consistent basis.\r\n\r\n\r\n\r\nNow that we have an application made, it’s time to get authenticated.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-28-first-post/first-post_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-15T16:20:36-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-12-second-post/",
    "title": "Post #3: Open Sesame",
    "description": "Maneuvering through Oauth2 for API access",
    "author": [
      {
        "name": "Jacob Corbridge",
        "url": "https://www.linkedin.com/in/jacobcorbridge/"
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nGetting Authorized seems to be the most complicated and annoying part of the data access process. Authorization requires seveal codes and ID’s, and access only lasts for 12 hours before the process needs to be repeated. If this process is ever to be automated, those issues will certainly need to be addressed. Lucky for us, that is for another time.\r\nTo begin, we need to import libraries and declare client info.\r\nfrom requests_oauthlib import OAuth2Sessions\r\nimport requests\r\nimport pandas as pd\r\nCLIENT_ID = 'place_client_id_here'\r\nCLIENT_SECRET = 'place_client_secret_here'\r\nCLIENT_REDIRECT_URI = 'place_client_redirect_uri_here'\r\nOnce we’ve done that. we will need to simply run some code provided. We will need the Token Grant URL and the Authorization URL to get and authorization code and access token. This chunk also displays our available scopes, or what level of data access we have.\r\nWELL_KNOWN_URL = 'https://signin.johndeere.com/oauth2/aus78tnlaysMraFhC1t7/.well-known/oauth-authorization-server'\r\n\r\n# Query the ./well-known OAuth URL and parse out the authorization URL, the token grant URL, and the available scopes\r\nwell_known_response = requests.get(WELL_KNOWN_URL)\r\nwell_known_info = json.loads(well_known_response.text)\r\n\r\nAUTHORIZATION_URL = well_known_info['authorization_endpoint']\r\nTOKEN_GRANT_URL = well_known_info['token_endpoint']\r\nAVAILABLE_SCOPES = str(' ').join(well_known_info['scopes_supported'])\r\n\r\nprint('Well Known Authorization URL - ' + AUTHORIZATION_URL)\r\nprint('Well Known Token Grant URL - ' + TOKEN_GRANT_URL)\r\nprint('Available Scopes - ' + AVAILABLE_SCOPES)\r\nNext we request an Authorization Code, and a link to Develop with Deere is provided.\r\nSCOPES_TO_REQUEST = {'offline_access', 'ag1', 'eq1', 'files', 'ag2', 'org1'}\r\nSTATE = \"Some Unique Identifier\"\r\noauth2_session = OAuth2Session(CLIENT_ID,  redirect_uri=CLIENT_REDIRECT_URI, scope=SCOPES_TO_REQUEST)\r\n\r\nauthorization_request, state = oauth2_session.authorization_url(AUTHORIZATION_URL, STATE)\r\nprint(\"Click on the following link to present the user with sign in form where they authenticate and approve access to your application.\")\r\nprint(authorization_request) \r\nHere we’ll copy the code provided within the URL of the website\r\n\r\n\r\n\r\nAnd paste it here.\r\n### Update the authorization code here\r\nAUTHORIZATION_CODE = 'place_authorization_code_here'\r\n\r\n# Leave the line below as-is. This is to make sure that you have update the AUTHORIZATION_CODE\r\nassert(AUTHORIZATION_CODE != 'place_authorization_code_here'), 'The AUTHORIZATION_CODE in this cell must be replaced by the authorization_code that you recieved'\r\n\r\n### Now that we have an authorization code, let's fetch an access and refresh token\r\ntoken_response = oauth2_session.fetch_token(TOKEN_GRANT_URL, code=AUTHORIZATION_CODE, client_secret=CLIENT_SECRET)\r\naccess_token = token_response['access_token']\r\nrefresh_token = token_response['refresh_token']\r\n\r\n# Also take note that the access token expiration time is returned.  When the access token expires, \r\n# you will want to use the refresh token to request a new access token (described later in this notebook)\r\naccess_token_expiration = token_response['expires_in']\r\n\r\nprint(\"Access Token: \" + access_token)\r\nprint(\"Refresh Token: \" + refresh_token)\r\nprint(\"Hours Token Is Valid: \" + str(int(access_token_expiration/60/60)))\r\nWe now have our access token, we’ll need that later when we attempt to make a call.\r\nWe are now authenticated and are free to invoke the API however we want. Thank goodness that is over. The code below will now connect to the sandbox API.\r\n### Create Connections Link\r\nURL = 'https://connections.deere.com/connections/' + CLIENT_ID + '&redirect_uri=' + CLIENT_REDIRECT_URI\r\n\r\nprint(URL)\r\nMYJOHNDEERE_V3_JSON_HEADERS = { 'Accept': 'application/vnd.deere.axiom.v3+json',\r\n                                'Content-Type': 'application/vnd.deere.axiom.v3+json'}\r\n\r\n# If your app happens to be already approved for production, then use the partnerapi.deere.com, otherwise stick with sandboxapi.deere.com\r\nAPI_CATALOG_URI = 'https://sandboxapi.deere.com/platform/'\r\n#API_CATALOG_URI = 'https://partnerapi.deere.com/platform/'\r\n\r\napi_catalog_response = oauth2_session.get(API_CATALOG_URI, headers=MYJOHNDEERE_V3_JSON_HEADERS)   \r\n# Test status code\r\napi_catalog_response.status_code\r\n# Test json response\r\napi_catalog_response.json()\r\n200 is a successful status response, and our response.json() code will create a json output. Before we move forward, we will need to do some housekeeping. The json code that is produced from this code is not quite what we want. There is a slight difference in output that we’ll want to address.The code below will clean up our previous output.\r\n### Clean Json Output with API Catalog\r\n\r\nlinks_array_from_api_catalog_response = api_catalog_response.json()['links']\r\nlinks_array_from_api_catalog_response\r\nNow we want to do the same with our organizations, not just the API catalog.\r\n### Make the org link for Org Response\r\n\r\norganizations_link = None\r\n\r\nfor link_object in links_array_from_api_catalog_response:\r\n  if(link_object['rel'] == 'organizations'):\r\n    organizations_link = link_object['uri']\r\n    break;\r\n    \r\norganizations_link\r\n### Org Response OAuth2 (messy output) \r\n\r\norganizations_response = oauth2_session.get(organizations_link, headers = MYJOHNDEERE_V3_JSON_HEADERS)\r\norganizations_response.json()\r\n### Clean Json Output with Org Response\r\n\r\nlinks_array_from_api_org_response = organizations_response.json()['values']\r\nlinks_array_from_api_org_response\r\nIf you want to do a deep dive on the purpose of this code, further exploration can be done here:\r\nhttps://colab.research.google.com/github/JohnDeere/DevelopWithDeereNotebooks/blob/master/Onboarding/myjohndeere-api-onboarding.ipynb#scrollTo=sNKiFKDHWAwZ)\r\n### Connections Link\r\n\r\nconnections_link = None\r\n\r\nfor link_object in links_array_from_api_org_response:\r\n  for links in link_object['links']:\r\n    if(links['rel'] == 'connections'):\r\n      connections_link = links['uri']\r\n      break;\r\n    \r\nprint(connections_link + \"&redirect_uri=\" + CLIENT_REDIRECT_URI)\r\nNow we’re going to want to take this output and put it into a dataframe so we can examine the data a little easier. Below is an example json output converted into table form. We’ll need a few more packages as well.\r\nimport numpy as np\r\nimport urllib3\r\nfrom pandas.io.json import json_normalize\r\nurl_example = [{'@type': 'Link',\r\n  'rel': 'currentUser',\r\n  'uri': 'https://sandboxapi.deere.com/platform/users/@currentUser'},\r\n {'@type': 'Link',\r\n  'rel': 'organizations',\r\n  'uri': 'https://sandboxapi.deere.com/platform/organizations'},\r\n {'@type': 'Link',\r\n  'rel': 'agencies',\r\n  'uri': 'https://sandboxapi.deere.com/platform/agencies'}]\r\n#cars = pd.read_json(url_cars)\r\njson_normalize(url_example)\r\nBeautiful.\r\nNext, we need to make an API call and download the file.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-12-second-post/second-post_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-07-15T15:58:42-06:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 711
  },
  {
    "path": "posts/2021-07-13-third-post/",
    "title": "Post #4: Make the call",
    "description": "Make an API Call and Download the Results",
    "author": [
      {
        "name": "Jacob Corbridge",
        "url": "https://www.linkedin.com/in/jacobcorbridge/"
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\r\nThere are an array of API calls specifically for John Deere Data. So many, in fact that it is easy to get overwhelmed. I should know, it happened to me. API’s shouldn’t scare us so much at this point. We’ve made it through the gauntlet of authentication and json output, now we can have a bit of fun. The John Deere API catalog displays the wide variety of calls we have available to us. they can be found here:\r\nhttps://developer-portal.deere.com/#/myjohndeere/api-inventory\r\nFor our test example, we’ll be accessing FieldOperations with the intent of downloading a Shapefile. For additional information on Shapefiles, refer here:\r\nhttps://developer-portal.deere.com/#/myjohndeere/field-operations/shapefiles-overview\r\nKeep in mind that the most important files are the .dbf, .shp, and .shx.\r\nHowever, we need to make our call first and obtain the data! We do that by using a GET request on the desired url using the following call:\r\nGET /organizations/{orgID}/fields/{fieldsID}/fieldOperations\r\n### Call to Field Operations to Obtain Operation ID\r\nimport requests\r\n \r\nurl = \"https://sandboxapi.deere.com/platform/organizations/317388/fields/34d5eaed-1508-4dc3-87a3-6d489767d8e1/fieldOperations\"\r\n\r\npayload={}\r\nheaders = {\r\n  'Accept': 'application/vnd.deere.axiom.v3+json',\r\n  'Authorization': 'Bearer [Place Access Token Here, No Brackets]'\r\n}\r\n \r\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\r\ndata = response.json()['values']\r\njson_normalize(data)\r\nRemember, when we include the access token it will be a large string of numbers, and it will be different every session. An example from a previous session looked like this:\r\n'Authorization': 'Bearer eyJraWQiOiJBRGE2aDV5MHZvZFpMbVdrQzlodTVvOWhWUGd5aTJjTjA2dFQ...\r\nThe string will actually come close to 1000 characters long, so do not be surprised with a very long access token string.\r\nAt the end of the previous chunk we took advantage of our json_normalize() function and were able to get this pretty output:\r\n\r\n\r\n\r\nNow that we have our raw data, we need to download all of this as a shapefile. This can be a complex process if you are importing a large amount of data. In our case we only have one field, but we need to be prepared in case that one day becomes necessary. In the previous chunk, we queried the Operations ID, a key piece. We will now use this ID as part of a new API call.\r\n### Download Shapefile Step 1: Insert Operations ID Into URL\r\nimport requests\r\n \r\nurl = \"https://sandboxapi.deere.com/platform/fieldOps/MzE3Mzg4XzViODQ1NjExY2QwOTk4MGQyMDdiYWVmZg\"\r\n\r\npayload={}\r\nheaders = {\r\n  'Accept': 'application/vnd.deere.axiom.v3+json',\r\n  'Authorization': 'Bearer [Insert Access Token Here and Remove Brackets']\r\n}\r\n \r\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\r\nprint(response.status_code)\r\n\r\nresponse1 = requests.request(\"GET\", url, headers=headers, data=payload)\r\nprint(response1)\r\nNow is where things get a little choppy, but it is a simple process. When we are attempting to download data, it can take a long time especially with shapefiles. As a result of that, the call can take up to 30 minutes to run depending on the size of the call. There are 4 API codes that are possible when we try to get our download. The first is 202, which essentially means “Working on it”. The next is a 307. This is a temporary redirect saying “We’re ready to download” In my experience this shouldn’t appear often, but our code accounts for it anyway. The final 2 are 406 and 200. 406 means a shapefile cannot be generated, most likely because a tillage operation was requested (tillage doesn’t work for some reason). 200, as previously mentioned, is a successful request and downloading can begin.\r\nThis is a small loop that will loop throught the download call until it is available to download. In my experience, there is little to no wait time. Our API call for this case is\r\nGET /fieldOps/{operationID} - Asynchronous Shapefile Download\r\n### Step 2: Loop until 307 or 200 code\r\n\r\n### Step 3: Download URL as zip when done\r\n\r\nimport urllib.request\r\nimport time\r\ntimeout = time.time() + 60*30\r\nsleeper = 5   \r\n\r\nwhile True:\r\n    url = \"https://sandboxapi.deere.com/platform/fieldOps/MzE3Mzg4XzViODQ1NjExY2QwOTk4MGQyMDdiYWVmZg\"\r\n\r\n    payload={}\r\n    headers = {\r\n    'Accept': 'application/vnd.deere.axiom.v3+json',\r\n    'Authorization': 'Bearer [Insert Access Token here and Remove Brackets]'\r\n    }\r\n \r\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\r\n\r\n    if response.status_code == 307 or 200:\r\n      urllib.request.urlretrieve(response.url, 'file.zip')\r\n      break\r\n\r\n    if time.time() > timeout: \r\n      break\r\n\r\n    time.sleep(sleeper) \r\n    sleeper = sleeper * 2\r\nWhen the file downloads, it should appear\r\nRefer to the following page for any questions\r\nhttps://developer-portal.deere.com/#/myjohndeere/field-operations/field-operations?hash=download-shapefile\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-13-third-post/third-post_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-15T16:10:27-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-15-post-4-pretty-colors/",
    "title": "Post #5: Shapes and Colors",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Jacob Corbridge",
        "url": "https://www.linkedin.com/in/jacobcorbridge/"
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\r\nNow that we have our Data downloaded as a zip file, we’ll need to dig into it to access what we want. The file types we want are the .shp, .shx, and .dbf. We need each in order to have a complete shapefile.\r\nFirst we need a few packages that allow us to work with zipfile\r\nimport zipfile\r\nfrom io import BytesIO\r\nimport io\r\nfrom google.colab import files\r\nimport re, os\r\nfrom urllib.request import urlopen\r\n### Put Zip in Memory\r\nf = zipfile.ZipFile([\"file_name.zip\", 'r')\r\n\r\n### Extract .shp, .shx and .dbf file from zip into doc folder\r\nlistOfFileNames = f.namelist()\r\n# Iterate over the file names\r\nfor fileName in listOfFileNames:\r\n    # Check filename endswith csv\r\n    if fileName.endswith('.shp') or fileName.endswith('.shx') or fileName.endswith('.dbf'):\r\n        # Extract a single file from zip\r\n         f.extract(fileName)\r\nThis code has taken our zip folder, extracted our 3 winners and placed them into a doc folder. Now we can work towards visualizing a field. We will be accomplishing this using the geopandas package. There are plenty of other visualization packages out there that may make the mapp look nicer, but our purpose is to make sure that the data actually works and can create an accurate map.\r\nFirst, we’re goint to import geopandas, declare a link to our files, read the file, then view the dataframe\r\n### Print Table Version of .shp\r\nimport geopandas as gpd\r\nimport geoplot\r\ndf_link = '/content/doc/BJ Christensen-Michaud-304-Potatoes.shp'\r\ndf = gpd.read_file(df_link)\r\ndf.head()\r\nTry a basic plot\r\n### Basic Field Plot\r\ndf.plot()\r\nI don’t love the look, but we’re on the rigth track. Let’s try to add some color.\r\nimport contextily as ctx\r\ndf = df.set_crs(epsg=4326)\r\ndf = df.to_crs(epsg=4326)\r\nsections = df.dissolve(by='SECTIONID', aggfunc='sum')\r\n\r\nsections.plot(column = 'NetYld', scheme='quantiles', cmap='viridis');\r\n\r\n\r\n\r\nAnd there we have it! While the plot is anything beautiful, the real win comes from the fact that we were able to access real John Deere Field Operation data and display it in a way that could potentially help a farmer make revenue altering decisions. The possibilities for future analysis is as endless as the supply with our newfound API connection.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-15-post-4-pretty-colors/post-4-pretty-colors_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-15T15:58:08-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
